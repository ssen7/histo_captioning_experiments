{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import h5py\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from dataloader import PreLoadedReps_v2,PreLoadedReps\n",
    "\n",
    "from models import Encoder, DecoderWithAttention,HIPT_LGP_FC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_path='/home/ss4yd/vision_transformer/captioning_vision_transformer/prepared_prelim_data_tokenized_cls256.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid</th>\n",
       "      <th>tissue_type</th>\n",
       "      <th>notes</th>\n",
       "      <th>svs_path</th>\n",
       "      <th>patch_path</th>\n",
       "      <th>reps_path</th>\n",
       "      <th>tokens</th>\n",
       "      <th>p_tokens</th>\n",
       "      <th>caplens</th>\n",
       "      <th>dtype</th>\n",
       "      <th>idx_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GTEX-1117F-0726</td>\n",
       "      <td>Heart - Atrial Appendage</td>\n",
       "      <td>2 pieces, no abnormalities</td>\n",
       "      <td>/project/GutIntelligenceLab/ss4yd/gtex_data/ac...</td>\n",
       "      <td>/project/GutIntelligenceLab/ss4yd/gtex_data/pa...</td>\n",
       "      <td>/project/GutIntelligenceLab/ss4yd/gtex_data/hi...</td>\n",
       "      <td>[2, pieces, ,, no, abnormalities]</td>\n",
       "      <td>[&lt;start&gt;, 2, pieces, ,, no, abnormalities, &lt;en...</td>\n",
       "      <td>7</td>\n",
       "      <td>train</td>\n",
       "      <td>[153, 1, 2, 3, 4, 5, 154, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GTEX-1117F-1026</td>\n",
       "      <td>Lung</td>\n",
       "      <td>2 pieces, moderate congestion/moderate to mark...</td>\n",
       "      <td>/project/GutIntelligenceLab/ss4yd/gtex_data/ac...</td>\n",
       "      <td>/project/GutIntelligenceLab/ss4yd/gtex_data/pa...</td>\n",
       "      <td>/project/GutIntelligenceLab/ss4yd/gtex_data/hi...</td>\n",
       "      <td>[2, pieces, ,, moderate, congestion/moderate, ...</td>\n",
       "      <td>[&lt;start&gt;, 2, pieces, ,, moderate, congestion/m...</td>\n",
       "      <td>10</td>\n",
       "      <td>train</td>\n",
       "      <td>[153, 1, 2, 3, 6, 152, 7, 8, 9, 154, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GTEX-1117F-1126</td>\n",
       "      <td>Liver</td>\n",
       "      <td>2 pieces, subtotal massive hepatic necrosis</td>\n",
       "      <td>/project/GutIntelligenceLab/ss4yd/gtex_data/ac...</td>\n",
       "      <td>/project/GutIntelligenceLab/ss4yd/gtex_data/pa...</td>\n",
       "      <td>/project/GutIntelligenceLab/ss4yd/gtex_data/hi...</td>\n",
       "      <td>[2, pieces, ,, subtotal, massive, hepatic, nec...</td>\n",
       "      <td>[&lt;start&gt;, 2, pieces, ,, subtotal, massive, hep...</td>\n",
       "      <td>9</td>\n",
       "      <td>train</td>\n",
       "      <td>[153, 1, 2, 3, 152, 152, 10, 11, 154, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GTEX-1117F-1226</td>\n",
       "      <td>Spleen</td>\n",
       "      <td>2 pieces, marked congestion/autolysis</td>\n",
       "      <td>/project/GutIntelligenceLab/ss4yd/gtex_data/ac...</td>\n",
       "      <td>/project/GutIntelligenceLab/ss4yd/gtex_data/pa...</td>\n",
       "      <td>/project/GutIntelligenceLab/ss4yd/gtex_data/hi...</td>\n",
       "      <td>[2, pieces, ,, marked, congestion/autolysis]</td>\n",
       "      <td>[&lt;start&gt;, 2, pieces, ,, marked, congestion/aut...</td>\n",
       "      <td>7</td>\n",
       "      <td>train</td>\n",
       "      <td>[153, 1, 2, 3, 8, 12, 154, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GTEX-1117F-1326</td>\n",
       "      <td>Adipose - Visceral (Omentum)</td>\n",
       "      <td>2 pieces, diffuse mesothelial hyperplasia; ~10...</td>\n",
       "      <td>/project/GutIntelligenceLab/ss4yd/gtex_data/ac...</td>\n",
       "      <td>/project/GutIntelligenceLab/ss4yd/gtex_data/pa...</td>\n",
       "      <td>/project/GutIntelligenceLab/ss4yd/gtex_data/hi...</td>\n",
       "      <td>[2, pieces, ,, diffuse, mesothelial, hyperplas...</td>\n",
       "      <td>[&lt;start&gt;, 2, pieces, ,, diffuse, mesothelial, ...</td>\n",
       "      <td>16</td>\n",
       "      <td>train</td>\n",
       "      <td>[153, 1, 2, 3, 13, 14, 15, 16, 17, 18, 152, 19...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               pid                   tissue_type  \\\n",
       "0  GTEX-1117F-0726      Heart - Atrial Appendage   \n",
       "1  GTEX-1117F-1026                          Lung   \n",
       "2  GTEX-1117F-1126                         Liver   \n",
       "3  GTEX-1117F-1226                        Spleen   \n",
       "4  GTEX-1117F-1326  Adipose - Visceral (Omentum)   \n",
       "\n",
       "                                               notes  \\\n",
       "0                         2 pieces, no abnormalities   \n",
       "1  2 pieces, moderate congestion/moderate to mark...   \n",
       "2        2 pieces, subtotal massive hepatic necrosis   \n",
       "3              2 pieces, marked congestion/autolysis   \n",
       "4  2 pieces, diffuse mesothelial hyperplasia; ~10...   \n",
       "\n",
       "                                            svs_path  \\\n",
       "0  /project/GutIntelligenceLab/ss4yd/gtex_data/ac...   \n",
       "1  /project/GutIntelligenceLab/ss4yd/gtex_data/ac...   \n",
       "2  /project/GutIntelligenceLab/ss4yd/gtex_data/ac...   \n",
       "3  /project/GutIntelligenceLab/ss4yd/gtex_data/ac...   \n",
       "4  /project/GutIntelligenceLab/ss4yd/gtex_data/ac...   \n",
       "\n",
       "                                          patch_path  \\\n",
       "0  /project/GutIntelligenceLab/ss4yd/gtex_data/pa...   \n",
       "1  /project/GutIntelligenceLab/ss4yd/gtex_data/pa...   \n",
       "2  /project/GutIntelligenceLab/ss4yd/gtex_data/pa...   \n",
       "3  /project/GutIntelligenceLab/ss4yd/gtex_data/pa...   \n",
       "4  /project/GutIntelligenceLab/ss4yd/gtex_data/pa...   \n",
       "\n",
       "                                           reps_path  \\\n",
       "0  /project/GutIntelligenceLab/ss4yd/gtex_data/hi...   \n",
       "1  /project/GutIntelligenceLab/ss4yd/gtex_data/hi...   \n",
       "2  /project/GutIntelligenceLab/ss4yd/gtex_data/hi...   \n",
       "3  /project/GutIntelligenceLab/ss4yd/gtex_data/hi...   \n",
       "4  /project/GutIntelligenceLab/ss4yd/gtex_data/hi...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0                  [2, pieces, ,, no, abnormalities]   \n",
       "1  [2, pieces, ,, moderate, congestion/moderate, ...   \n",
       "2  [2, pieces, ,, subtotal, massive, hepatic, nec...   \n",
       "3       [2, pieces, ,, marked, congestion/autolysis]   \n",
       "4  [2, pieces, ,, diffuse, mesothelial, hyperplas...   \n",
       "\n",
       "                                            p_tokens  caplens  dtype  \\\n",
       "0  [<start>, 2, pieces, ,, no, abnormalities, <en...        7  train   \n",
       "1  [<start>, 2, pieces, ,, moderate, congestion/m...       10  train   \n",
       "2  [<start>, 2, pieces, ,, subtotal, massive, hep...        9  train   \n",
       "3  [<start>, 2, pieces, ,, marked, congestion/aut...        7  train   \n",
       "4  [<start>, 2, pieces, ,, diffuse, mesothelial, ...       16  train   \n",
       "\n",
       "                                          idx_tokens  \n",
       "0  [153, 1, 2, 3, 4, 5, 154, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "1  [153, 1, 2, 3, 6, 152, 7, 8, 9, 154, 0, 0, 0, ...  \n",
       "2  [153, 1, 2, 3, 152, 152, 10, 11, 154, 0, 0, 0,...  \n",
       "3  [153, 1, 2, 3, 8, 12, 154, 0, 0, 0, 0, 0, 0, 0...  \n",
       "4  [153, 1, 2, 3, 13, 14, 15, 16, 17, 18, 152, 19...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_pickle(df_path).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 384])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds=PreLoadedReps(df_path,'val')\n",
    "\n",
    "ds.__getitem__(0)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "            PreLoadedReps_v2(df_path,'train'),\n",
    "            batch_size=1, shuffle=True, num_workers=1, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([21, 256, 384])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset.__getitem__(0)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Patches: 196\n",
      "Loading Pretrained Local VIT model...\n",
      "Done!\n",
      "Freezing Pretrained Local VIT model\n",
      "Done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HIPT_LGP_FC(\n",
       "  (local_vit): VisionTransformer4K(\n",
       "    (phi): Sequential(\n",
       "      (0): Linear(in_features=384, out_features=192, bias=True)\n",
       "      (1): GELU()\n",
       "      (2): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (blocks): ModuleList(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "    (head): Identity()\n",
       "  )\n",
       "  (global_phi): Sequential(\n",
       "    (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.25, inplace=False)\n",
       "  )\n",
       "  (global_transformer): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (dropout): Dropout(p=0.25, inplace=False)\n",
       "        (linear2): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.25, inplace=False)\n",
       "        (dropout2): Dropout(p=0.25, inplace=False)\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (dropout): Dropout(p=0.25, inplace=False)\n",
       "        (linear2): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.25, inplace=False)\n",
       "        (dropout2): Dropout(p=0.25, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (global_attn_pool): Attn_Net_Gated(\n",
       "    (attention_a): Sequential(\n",
       "      (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "    )\n",
       "    (attention_b): Sequential(\n",
       "      (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "      (1): Sigmoid()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "    )\n",
       "    (attention_c): Linear(in_features=192, out_features=1, bias=True)\n",
       "  )\n",
       "  (global_rho): Sequential(\n",
       "    (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.25, inplace=False)\n",
       "  )\n",
       "  (classifier): Linear(in_features=192, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder =HIPT_LGP_FC(freeze_4k=True, pretrain_4k='vit4k_xs_dino', n_classes=2)\n",
    "encoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([48, 256, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=bicubic is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/site-packages/torch/nn/functional.py:3679: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for i, (imgs, caps, caplens) in enumerate(train_loader):\n",
    "#     data_time.update(time.time() - start)\n",
    "\n",
    "    # Move to GPU, if available\n",
    "    imgs = imgs.to(device).squeeze(0)\n",
    "    caps = caps.to(device)\n",
    "    caplens = caplens.to(device)\n",
    "    \n",
    "    print(imgs.shape)\n",
    "\n",
    "    # Forward prop.\n",
    "    imgs = encoder(imgs)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0275, 0.0205, 0.0213, 0.0257, 0.0224, 0.0176, 0.0203, 0.0164, 0.0213,\n",
       "         0.0169, 0.0193, 0.0233, 0.0203, 0.0238, 0.0191, 0.0229, 0.0143, 0.0249,\n",
       "         0.0220, 0.0260, 0.0246, 0.0222, 0.0251, 0.0234, 0.0204, 0.0211, 0.0186,\n",
       "         0.0214, 0.0194, 0.0235, 0.0168, 0.0206, 0.0252, 0.0206, 0.0185, 0.0171,\n",
       "         0.0207, 0.0171, 0.0248, 0.0187, 0.0269, 0.0183, 0.0171, 0.0172, 0.0191,\n",
       "         0.0210, 0.0159, 0.0190]], device='cuda:0', grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 1.10.0",
   "language": "python",
   "name": "pytorch-1.10.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
