{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "from dataloader import *\n",
    "from utils import *\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from rouge_score import rouge_scorer\n",
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "# data_folder = '/media/ssd/caption data'  # folder with data files saved by create_input_files.py\n",
    "# data_name = 'coco_5_cap_per_img_5_min_word_freq'  # base name shared by data files\n",
    "df_path='/home/ss4yd/vision_transformer/captioning_vision_transformer/prepared_prelim_data_tokenized_cls256.pickle'\n",
    "data_name = 'hipt_captioning_task'  # base name shared by data files\n",
    "word_map=read_obj('./word_map_cls256.pickle')\n",
    "\n",
    "checkpoint = './BEST_checkpoint_hipt_captioning_task.pth.tar'  # model checkpoint\n",
    "# word_map_file = '/media/ssd/caption data/WORDMAP_coco_5_cap_per_img_5_min_word_freq.json'  # word map, ensure it's the same the data was encoded with and the model was trained with\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # sets device for model and PyTorch tensors\n",
    "cudnn.benchmark = True  # set to true only if inputs to model are fixed size; otherwise lot of computational overhead\n",
    "\n",
    "# Load model\n",
    "checkpoint = torch.load(checkpoint)\n",
    "decoder = checkpoint['decoder']\n",
    "decoder = decoder.to(device)\n",
    "decoder.eval()\n",
    "encoder = checkpoint['encoder']\n",
    "encoder = encoder.to(device)\n",
    "encoder.eval()\n",
    "\n",
    "# Load word map (word2ix)\n",
    "# with open(word_map_file, 'r') as j:\n",
    "#     word_map = json.load(j)\n",
    "rev_word_map = {v: k for k, v in word_map.items()}\n",
    "vocab_size = len(word_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EVALUATING AT BEAM SIZE 1:   0%|          | 0/5 [00:00<?, ?it/s]/usr/local/lib/python3.8/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=bicubic is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/site-packages/torch/nn/functional.py:3679: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\n",
      "EVALUATING AT BEAM SIZE 1: 100%|██████████| 5/5 [00:01<00:00,  3.74it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.11288196593337896"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beam_size=1\n",
    "loader = torch.utils.data.DataLoader(\n",
    "            PreLoadedReps_v2(df_path,'test'),\n",
    "            batch_size=1, shuffle=True, num_workers=1, pin_memory=True)\n",
    "\n",
    "# TODO: Batched Beam Search\n",
    "# Therefore, do not use a batch_size greater than 1 - IMPORTANT!\n",
    "\n",
    "# Lists to store references (true captions), and hypothesis (prediction) for each image\n",
    "# If for n images, we have n hypotheses, and references a, b, c... for each image, we need -\n",
    "# references = [[ref1a, ref1b, ref1c], [ref2a, ref2b], ...], hypotheses = [hyp1, hyp2, ...]\n",
    "references = list()\n",
    "hypotheses = list()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # For each image\n",
    "    for i, (image, caps, caplens, allcaps) in enumerate(\n",
    "            tqdm(loader, desc=\"EVALUATING AT BEAM SIZE \" + str(beam_size))):\n",
    "\n",
    "        k = beam_size\n",
    "\n",
    "        # Move to GPU device, if available\n",
    "        image = image.to(device).squeeze(0)  # (1, 3, 256, 256)\n",
    "#         print(image.shape)\n",
    "#         print([rev_word_map[i] for i in caps[0].numpy()])\n",
    "        # Encode\n",
    "        encoder_out,_ = encoder(image)  # (1, enc_image_size, enc_image_size, encoder_dim)\n",
    "        enc_image_size = encoder_out.size(1)\n",
    "        encoder_dim = encoder_out.size(-1)\n",
    "\n",
    "        # Flatten encoding\n",
    "        encoder_out = encoder_out.view(1, -1, encoder_dim)  # (1, num_pixels, encoder_dim)\n",
    "        num_pixels = encoder_out.size(1)\n",
    "\n",
    "        # We'll treat the problem as having a batch size of k\n",
    "        encoder_out = encoder_out.expand(k, num_pixels, encoder_dim)  # (k, num_pixels, encoder_dim)\n",
    "\n",
    "        # Tensor to store top k previous words at each step; now they're just <start>\n",
    "        k_prev_words = torch.LongTensor([[word_map['<start>']]] * k).to(device)  # (k, 1)\n",
    "\n",
    "        # Tensor to store top k sequences; now they're just <start>\n",
    "        seqs = k_prev_words  # (k, 1)\n",
    "\n",
    "        # Tensor to store top k sequences' scores; now they're just 0\n",
    "        top_k_scores = torch.zeros(k, 1).to(device)  # (k, 1)\n",
    "\n",
    "        # Lists to store completed sequences and scores\n",
    "        complete_seqs = list()\n",
    "        complete_seqs_scores = list()\n",
    "\n",
    "        # Start decoding\n",
    "        step = 1\n",
    "        h, c = decoder.init_hidden_state(encoder_out)\n",
    "\n",
    "        # s is a number less than or equal to k, because sequences are removed from this process once they hit <end>\n",
    "        while True:\n",
    "            embeddings = decoder.embedding(k_prev_words).squeeze(1)  # (s, embed_dim)\n",
    "\n",
    "            awe, _ = decoder.attention(encoder_out, h)  # (s, encoder_dim), (s, num_pixels)\n",
    "\n",
    "            gate = decoder.sigmoid(decoder.f_beta(h))  # gating scalar, (s, encoder_dim)\n",
    "            awe = gate * awe\n",
    "\n",
    "            h, c = decoder.decode_step(torch.cat([embeddings, awe], dim=1), (h, c))  # (s, decoder_dim)\n",
    "\n",
    "            scores = decoder.fc(h)  # (s, vocab_size)\n",
    "            scores = F.log_softmax(scores, dim=1)\n",
    "\n",
    "            # Add\n",
    "            scores = top_k_scores.expand_as(scores) + scores  # (s, vocab_size)\n",
    "\n",
    "            # For the first step, all k points will have the same scores (since same k previous words, h, c)\n",
    "            if step == 1:\n",
    "                top_k_scores, top_k_words = scores[0].topk(k, 0, True, True)  # (s)\n",
    "                \n",
    "            else:\n",
    "                # Unroll and find top scores, and their unrolled indices\n",
    "                top_k_scores, top_k_words = scores.view(-1).topk(k, 0, True, True)  # (s)\n",
    "            \n",
    "            # Convert unrolled indices to actual indices of scores\n",
    "            prev_word_inds = torch.div(top_k_words, vocab_size, rounding_mode='floor')  # (s)\n",
    "            next_word_inds = top_k_words % vocab_size  # (s)\n",
    "            \n",
    "#             print(prev_word_inds,next_word_inds)\n",
    "\n",
    "            # Add new words to sequences\n",
    "            seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)  # (s, step+1)\n",
    "\n",
    "            # Which sequences are incomplete (didn't reach <end>)?\n",
    "            incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if\n",
    "                               next_word != word_map['<end>']]\n",
    "#             print(incomplete_inds)\n",
    "            complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n",
    "\n",
    "            # Set aside complete sequences\n",
    "            if len(complete_inds) > 0:\n",
    "                complete_seqs.extend(seqs[complete_inds].tolist())\n",
    "                complete_seqs_scores.extend(top_k_scores[complete_inds])\n",
    "            k -= len(complete_inds)  # reduce beam length accordingly\n",
    "\n",
    "            # Proceed with incomplete sequences\n",
    "            if k == 0:\n",
    "                break\n",
    "            seqs = seqs[incomplete_inds]\n",
    "            h = h[prev_word_inds[incomplete_inds]]\n",
    "            c = c[prev_word_inds[incomplete_inds]]\n",
    "            encoder_out = encoder_out[prev_word_inds[incomplete_inds]]\n",
    "            top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n",
    "            k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n",
    "\n",
    "            # Break if things have been going on too long\n",
    "            if step > 200:\n",
    "                break\n",
    "            step += 1\n",
    "\n",
    "        i = complete_seqs_scores.index(max(complete_seqs_scores))\n",
    "        seq = complete_seqs[i]\n",
    "\n",
    "        # References\n",
    "        img_caps = allcaps[0].tolist()\n",
    "        img_captions = list(\n",
    "            map(lambda c: [w for w in c if w not in {word_map['<start>'], word_map['<end>'], word_map['<pad>']}],\n",
    "                img_caps))  # remove <start> and pads\n",
    "        references.append(img_captions)\n",
    "\n",
    "        # Hypotheses\n",
    "        hypotheses.append([w for w in seq if w not in {word_map['<start>'], word_map['<end>'], word_map['<pad>']}])\n",
    "\n",
    "        assert len(references) == len(hypotheses)\n",
    "\n",
    "# Calculate BLEU-4 scores\n",
    "bleu4 = corpus_bleu(references, hypotheses)\n",
    "bleu4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     beam_size = 1\n",
    "#     print(\"\\nBLEU-4 score @ beam size of %d is %.4f.\" % (beam_size, evaluate(beam_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 192])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder(image)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=bicubic is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/site-packages/torch/nn/functional.py:3679: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(16, device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(encoder(image)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start>']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[rev_word_map[i] for i in seqs[0].cpu().numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [rev_word_map[i] for i in pd.read_pickle('./prepared_prelim_data_tokenized.pickle')['idx_tokens'][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "scores = scorer.score('The quick brown fox jumps over the lazy dog',\n",
    "                      'The quick brown dog jumps on the log.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def return_string_list(sll):\n",
    "    if type(sll[0][0])==list:\n",
    "        references_str=[x[0] for x in sll]\n",
    "    else:\n",
    "        references_str=sll\n",
    "    references_str=[[rev_word_map[x] for x in sl] for sl in references_str]\n",
    "    references_str=[' '.join(x) for x in references_str]\n",
    "    return references_str\n",
    "\n",
    "hypotheses_str=return_string_list(hypotheses)\n",
    "references_str=return_string_list(references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2 pieces , <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>',\n",
       " '6 pieces , mild atherosclerosis , some attached <unk>')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypotheses_str[0], references_str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(references_str)):\n",
    "#     print(scorer.score(references_str[i], hypotheses_str[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = Rouge(metrics=['rouge-l'])\n",
    "scores = rouge.get_scores(hypotheses_str, references_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5235714285714286"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([x['rouge-l']['r'] for x in scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 256, 384])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 192, 192]' is invalid for input of size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [16], line 59\u001b[0m\n\u001b[1;32m     55\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39membedding(k_prev_words)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (s, embed_dim)\u001b[39;00m\n\u001b[1;32m     57\u001b[0m awe, alpha \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mattention(encoder_out, h)  \u001b[38;5;66;03m# (s, encoder_dim), (s, num_pixels)\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[43malpha\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_image_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_image_size\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (s, enc_image_size, enc_image_size)\u001b[39;00m\n\u001b[1;32m     61\u001b[0m gate \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39msigmoid(decoder\u001b[38;5;241m.\u001b[39mf_beta(h))  \u001b[38;5;66;03m# gating scalar, (s, encoder_dim)\u001b[39;00m\n\u001b[1;32m     62\u001b[0m awe \u001b[38;5;241m=\u001b[39m gate \u001b[38;5;241m*\u001b[39m awe\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 192, 192]' is invalid for input of size 3"
     ]
    }
   ],
   "source": [
    "k = 3\n",
    "vocab_size = len(word_map)\n",
    "\n",
    "# Read image and process\n",
    "# img = imread(image_path)\n",
    "# if len(img.shape) == 2:\n",
    "#     img = img[:, :, np.newaxis]\n",
    "#     img = np.concatenate([img, img, img], axis=2)\n",
    "# img = imresize(img, (256, 256))\n",
    "# img = img.transpose(2, 0, 1)\n",
    "# img = img / 255.\n",
    "# img = torch.FloatTensor(img).to(device)\n",
    "# normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                  std=[0.229, 0.224, 0.225])\n",
    "# transform = transforms.Compose([normalize])\n",
    "# image = transform(img)  # (3, 256, 256)\n",
    "\n",
    "# Encode\n",
    "# image = image.unsqueeze(0)  # (1, 3, 256, 256)\n",
    "encoder_out,_ = encoder(image)  # (1, enc_image_size, enc_image_size, encoder_dim)\n",
    "enc_image_size = encoder_out.size(1)\n",
    "encoder_dim = encoder_out.size(-1)\n",
    "\n",
    "# Flatten encoding\n",
    "encoder_out = encoder_out.view(1, -1, encoder_dim)  # (1, num_pixels, encoder_dim)\n",
    "num_pixels = encoder_out.size(1)\n",
    "\n",
    "# We'll treat the problem as having a batch size of k\n",
    "encoder_out = encoder_out.expand(k, num_pixels, encoder_dim)  # (k, num_pixels, encoder_dim)\n",
    "\n",
    "# Tensor to store top k previous words at each step; now they're just <start>\n",
    "k_prev_words = torch.LongTensor([[word_map['<start>']]] * k).to(device)  # (k, 1)\n",
    "\n",
    "# Tensor to store top k sequences; now they're just <start>\n",
    "seqs = k_prev_words  # (k, 1)\n",
    "\n",
    "# Tensor to store top k sequences' scores; now they're just 0\n",
    "top_k_scores = torch.zeros(k, 1).to(device)  # (k, 1)\n",
    "\n",
    "# Tensor to store top k sequences' alphas; now they're just 1s\n",
    "seqs_alpha = torch.ones(k, 1, enc_image_size, enc_image_size).to(device)  # (k, 1, enc_image_size, enc_image_size)\n",
    "\n",
    "# Lists to store completed sequences, their alphas and scores\n",
    "complete_seqs = list()\n",
    "complete_seqs_alpha = list()\n",
    "complete_seqs_scores = list()\n",
    "\n",
    "# Start decoding\n",
    "step = 1\n",
    "h, c = decoder.init_hidden_state(encoder_out)\n",
    "\n",
    "# s is a number less than or equal to k, because sequences are removed from this process once they hit <end>\n",
    "while True:\n",
    "\n",
    "    embeddings = decoder.embedding(k_prev_words).squeeze(1)  # (s, embed_dim)\n",
    "\n",
    "    awe, alpha = decoder.attention(encoder_out, h)  # (s, encoder_dim), (s, num_pixels)\n",
    "\n",
    "    alpha = alpha.view(-1, enc_image_size, enc_image_size)  # (s, enc_image_size, enc_image_size)\n",
    "\n",
    "    gate = decoder.sigmoid(decoder.f_beta(h))  # gating scalar, (s, encoder_dim)\n",
    "    awe = gate * awe\n",
    "\n",
    "    h, c = decoder.decode_step(torch.cat([embeddings, awe], dim=1), (h, c))  # (s, decoder_dim)\n",
    "\n",
    "    scores = decoder.fc(h)  # (s, vocab_size)\n",
    "    scores = F.log_softmax(scores, dim=1)\n",
    "\n",
    "    # Add\n",
    "    scores = top_k_scores.expand_as(scores) + scores  # (s, vocab_size)\n",
    "\n",
    "    # For the first step, all k points will have the same scores (since same k previous words, h, c)\n",
    "    if step == 1:\n",
    "        top_k_scores, top_k_words = scores[0].topk(k, 0, True, True)  # (s)\n",
    "    else:\n",
    "        # Unroll and find top scores, and their unrolled indices\n",
    "        top_k_scores, top_k_words = scores.view(-1).topk(k, 0, True, True)  # (s)\n",
    "\n",
    "    # Convert unrolled indices to actual indices of scores\n",
    "    prev_word_inds = torch.div(top_k_words, vocab_size, rounding_mode='floor')  # (s)\n",
    "    next_word_inds = top_k_words % vocab_size  # (s)\n",
    "\n",
    "    # Add new words to sequences, alphas\n",
    "    seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)  # (s, step+1)\n",
    "    seqs_alpha = torch.cat([seqs_alpha[prev_word_inds], alpha[prev_word_inds].unsqueeze(1)],\n",
    "                           dim=1)  # (s, step+1, enc_image_size, enc_image_size)\n",
    "\n",
    "    # Which sequences are incomplete (didn't reach <end>)?\n",
    "    incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if\n",
    "                       next_word != word_map['<end>']]\n",
    "    complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n",
    "\n",
    "    # Set aside complete sequences\n",
    "    if len(complete_inds) > 0:\n",
    "        complete_seqs.extend(seqs[complete_inds].tolist())\n",
    "        complete_seqs_alpha.extend(seqs_alpha[complete_inds].tolist())\n",
    "        complete_seqs_scores.extend(top_k_scores[complete_inds])\n",
    "    k -= len(complete_inds)  # reduce beam length accordingly\n",
    "\n",
    "    # Proceed with incomplete sequences\n",
    "    if k == 0:\n",
    "        break\n",
    "    seqs = seqs[incomplete_inds]\n",
    "    seqs_alpha = seqs_alpha[incomplete_inds]\n",
    "    h = h[prev_word_inds[incomplete_inds]]\n",
    "    c = c[prev_word_inds[incomplete_inds]]\n",
    "    encoder_out = encoder_out[prev_word_inds[incomplete_inds]]\n",
    "    top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n",
    "    k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n",
    "\n",
    "    # Break if things have been going on too long\n",
    "    if step > 50:\n",
    "        break\n",
    "    step += 1\n",
    "\n",
    "i = complete_seqs_scores.index(max(complete_seqs_scores))\n",
    "seq = complete_seqs[i]\n",
    "alphas = complete_seqs_alpha[i]\n",
    "\n",
    "seq, alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[rev_word_map[i] for i in seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[rev_word_map[i] for i in complete_seqs[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caps[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[rev_word_map[i] for i in caps[0].numpy()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 1.10.0",
   "language": "python",
   "name": "pytorch-1.10.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
