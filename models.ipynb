{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from dataloader import *\n",
    "from utils import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torchvision\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "\n",
    "from transformers import BertTokenizer, AutoTokenizer, AutoModel, BertModel, AutoConfig\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import pdb\n",
    "from os.path import join\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "from HIPT.supervised_subtyping.models.model_utils import *\n",
    "\n",
    "import sys\n",
    "# sys.path.append('../HIPT_4K/')\n",
    "from HIPT.HIPT_4K.vision_transformer4k import vit4k_xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HIPT_None_FC(nn.Module):\n",
    "    def __init__(self, path_input_dim=384, size_arg = \"small\", dropout=0.25, n_classes=2):\n",
    "        super(HIPT_None_FC, self).__init__()\n",
    "        self.size_dict_path = {\"small\": [path_input_dim, 256, 256], \"big\": [path_input_dim, 512, 384]}\n",
    "        size = self.size_dict_path[size_arg]\n",
    "\n",
    "        ### Local Aggregation\n",
    "        self.local_phi = nn.Sequential(\n",
    "            nn.Linear(size[0], size[1]), nn.ReLU(), nn.Dropout(0.25),\n",
    "        )\n",
    "        self.local_attn_pool = Attn_Net_Gated(L=size[1], D=size[1], dropout=0.25, n_classes=1)\n",
    "        \n",
    "        ### Global Aggregation\n",
    "        self.global_phi = nn.Sequential(\n",
    "            nn.Linear(size[1], size[1]), nn.ReLU(), nn.Dropout(0.25),\n",
    "        )\n",
    "        self.global_attn_pool = Attn_Net_Gated(L=size[1], D=size[1], dropout=0.25, n_classes=1)\n",
    "        self.global_rho = nn.Sequential(*[nn.Linear(size[1], size[1]), nn.ReLU(), nn.Dropout(0.25)])\n",
    "        self.classifier = nn.Linear(size[1], n_classes)\n",
    "\n",
    "\n",
    "    def forward(self, h, **kwargs):\n",
    "        x_256 = h\n",
    "\n",
    "        ### Local\n",
    "        h_256 = self.local_phi(x_256)\n",
    "        A_256, h_256 = self.local_attn_pool(h_256)  \n",
    "        A_256 = A_256.squeeze(dim=2) # A = torch.transpose(A, 1, 0)\n",
    "        A_256 = F.softmax(A_256, dim=1) \n",
    "        h_4096 = torch.bmm(A_256.unsqueeze(dim=1), h_256).squeeze(dim=1)\n",
    "        \n",
    "        ### Global\n",
    "        h_4096 = self.global_phi(h_4096)\n",
    "        A_4096, h_4096 = self.global_attn_pool(h_4096)  \n",
    "        A_4096 = torch.transpose(A_4096, 1, 0)\n",
    "        A_4096 = F.softmax(A_4096, dim=1) \n",
    "        h_path = torch.mm(A_4096, h_4096)\n",
    "        h_path = self.global_rho(h_path)\n",
    "        logits = self.classifier(h_path)\n",
    "\n",
    "        Y_hat = torch.topk(logits, 1, dim = 1)[1]\n",
    "        Y_prob = F.softmax(logits, dim = 1)\n",
    "\n",
    "        return logits, Y_prob, Y_hat, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HIPT_LGP_FC(nn.Module):\n",
    "    def __init__(self, path_input_dim=384,  size_arg = \"small\", dropout=0.25, n_classes=4,\n",
    "     pretrain_4k='None', freeze_4k=False, pretrain_WSI='None', freeze_WSI=False):\n",
    "        super(HIPT_LGP_FC, self).__init__()\n",
    "        self.size_dict_path = {\"small\": [384, 192, 192], \"big\": [1024, 512, 384]}\n",
    "        #self.fusion = fusion\n",
    "        size = self.size_dict_path[size_arg]\n",
    "\n",
    "        ### Local Aggregation\n",
    "        self.local_vit = vit4k_xs()\n",
    "        if pretrain_4k != 'None':\n",
    "            print(\"Loading Pretrained Local VIT model...\",)\n",
    "            state_dict = torch.load('../../HIPT_4K/Checkpoints/%s.pth' % pretrain_4k, map_location='cpu')['teacher']\n",
    "            state_dict = {k.replace('module.', \"\"): v for k, v in state_dict.items()}\n",
    "            state_dict = {k.replace('backbone.', \"\"): v for k, v in state_dict.items()}\n",
    "            missing_keys, unexpected_keys = self.local_vit.load_state_dict(state_dict, strict=False)\n",
    "            print(\"Done!\")\n",
    "        if freeze_4k:\n",
    "            print(\"Freezing Pretrained Local VIT model\")\n",
    "            for param in self.local_vit.parameters():\n",
    "                param.requires_grad = False\n",
    "            print(\"Done\")\n",
    "\n",
    "        ### Global Aggregation\n",
    "        self.pretrain_WSI = pretrain_WSI\n",
    "        if pretrain_WSI != 'None':\n",
    "            pass\n",
    "        else:\n",
    "            self.global_phi = nn.Sequential(nn.Linear(192, 192), nn.ReLU(), nn.Dropout(0.25))\n",
    "            self.global_transformer = nn.TransformerEncoder(\n",
    "                nn.TransformerEncoderLayer(\n",
    "                    d_model=192, nhead=3, dim_feedforward=192, dropout=0.25, activation='relu'\n",
    "                ), \n",
    "                num_layers=2\n",
    "            )\n",
    "            self.global_attn_pool = Attn_Net_Gated(L=size[1], D=size[1], dropout=0.25, n_classes=1)\n",
    "            self.global_rho = nn.Sequential(*[nn.Linear(size[1], size[1]), nn.ReLU(), nn.Dropout(0.25)])\n",
    "\n",
    "        self.classifier = nn.Linear(size[1], n_classes)\n",
    "        \n",
    "\n",
    "    def forward(self, x_256, **kwargs):\n",
    "        ### Local\n",
    "        h_4096 = self.local_vit(x_256.unfold(1, 16, 16).transpose(1,2))\n",
    "        \n",
    "        ### Global\n",
    "        if self.pretrain_WSI != 'None':\n",
    "            h_WSI = self.global_vit(h_4096.unsqueeze(dim=0))\n",
    "        else:\n",
    "            h_4096 = self.global_phi(h_4096)\n",
    "            h_4096 = self.global_transformer(h_4096.unsqueeze(1)).squeeze(1)\n",
    "            A_4096, h_4096 = self.global_attn_pool(h_4096)  \n",
    "            A_4096 = torch.transpose(A_4096, 1, 0)\n",
    "            A_4096 = F.softmax(A_4096, dim=1) \n",
    "            h_path = torch.mm(A_4096, h_4096)\n",
    "            h_WSI = self.global_rho(h_path)\n",
    "\n",
    "        logits = self.classifier(h_WSI)\n",
    "        Y_hat = torch.topk(logits, 1, dim = 1)[1]\n",
    "        return logits, F.softmax(logits, dim=1), Y_hat, None, None\n",
    "\n",
    "\n",
    "    def relocate(self):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        if torch.cuda.device_count() >= 1:\n",
    "            device_ids = list(range(torch.cuda.device_count()))\n",
    "            self.local_vit = nn.DataParallel(self.local_vit, device_ids=device_ids).to('cuda:0')\n",
    "            if self.pretrain_WSI != 'None':\n",
    "                self.global_vit = nn.DataParallel(self.global_vit, device_ids=device_ids).to('cuda:0')\n",
    "\n",
    "        if self.pretrain_WSI == 'None':\n",
    "            self.global_phi = self.global_phi.to(device)\n",
    "            self.global_transformer = self.global_transformer.to(device)\n",
    "            self.global_attn_pool = self.global_attn_pool.to(device)\n",
    "            self.global_rho = self.global_rho.to(device)\n",
    "\n",
    "        self.classifier = self.classifier.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Patches: 196\n",
      "Freezing Pretrained Local VIT model\n",
      "Done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HIPT_LGP_FC(\n",
       "  (local_vit): VisionTransformer4K(\n",
       "    (phi): Sequential(\n",
       "      (0): Linear(in_features=384, out_features=192, bias=True)\n",
       "      (1): GELU()\n",
       "      (2): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (blocks): ModuleList(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "    (head): Identity()\n",
       "  )\n",
       "  (global_phi): Sequential(\n",
       "    (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.25, inplace=False)\n",
       "  )\n",
       "  (global_transformer): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (dropout): Dropout(p=0.25, inplace=False)\n",
       "        (linear2): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.25, inplace=False)\n",
       "        (dropout2): Dropout(p=0.25, inplace=False)\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (dropout): Dropout(p=0.25, inplace=False)\n",
       "        (linear2): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.25, inplace=False)\n",
       "        (dropout2): Dropout(p=0.25, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (global_attn_pool): Attn_Net_Gated(\n",
       "    (attention_a): Sequential(\n",
       "      (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "    )\n",
       "    (attention_b): Sequential(\n",
       "      (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "      (1): Sigmoid()\n",
       "      (2): Dropout(p=0.25, inplace=False)\n",
       "    )\n",
       "    (attention_c): Linear(in_features=192, out_features=1, bias=True)\n",
       "  )\n",
       "  (global_rho): Sequential(\n",
       "    (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.25, inplace=False)\n",
       "  )\n",
       "  (classifier): Linear(in_features=192, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HIPT_LGP_FC(freeze_4k=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_map=read_obj('./word_map.pickle')\n",
    "idx2token={k:v for v,k in word_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([192])\n"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim=192,encoded_image_size=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc_image_size = encoded_image_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.fc = nn.Linear(embed_dim, embed_dim) # pretrained HIPT reps\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, reps):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "        :param images: images, a tensor of dimensions (batch_size, 3, image_size, image_size)\n",
    "        :return: encoded images\n",
    "        \"\"\"\n",
    "        out = self.relu(self.fc(reps)) # (batch_size, 2048, image_size/32, image_size/32)\n",
    "#         out = self.adaptive_pool(out)  # (batch_size, 2048, encoded_image_size, encoded_image_size)\n",
    "#         out = out.permute(0, 2, 3, 1)  # (batch_size, encoded_image_size, encoded_image_size, 2048)\n",
    "        return out\n",
    "\n",
    "df_path='/home/ss4yd/vision_transformer/captioning_vision_transformer/prepared_prelim_data_tokenized.pickle'\n",
    "ds=PreLoadedReps(df_path,'val')\n",
    "\n",
    "encoder=Encoder()\n",
    "\n",
    "rep=ds.__getitem__(0)[0]\n",
    "print(rep.shape)\n",
    "\n",
    "enc_out=encoder(rep.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention Network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
    "        \"\"\"\n",
    "        :param encoder_dim: feature size of encoded images\n",
    "        :param decoder_dim: size of decoder's RNN\n",
    "        :param attention_dim: size of the attention network\n",
    "        \"\"\"\n",
    "        super(Attention, self).__init__()\n",
    "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # linear layer to transform encoded image\n",
    "        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # linear layer to transform decoder's output\n",
    "        self.full_att = nn.Linear(attention_dim, 1)  # linear layer to calculate values to be softmax-ed\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)  # softmax layer to calculate weights\n",
    "\n",
    "    def forward(self, encoder_out, decoder_hidden):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n",
    "        :param decoder_hidden: previous decoder output, a tensor of dimension (batch_size, decoder_dim)\n",
    "        :return: attention weighted encoding, weights\n",
    "        \"\"\"\n",
    "        att1 = self.encoder_att(encoder_out)  # (batch_size, num_pixels, attention_dim)\n",
    "        att2 = self.decoder_att(decoder_hidden)  # (batch_size, attention_dim)\n",
    "        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)  # (batch_size, num_pixels)\n",
    "        alpha = self.softmax(att)  # (batch_size, num_pixels)\n",
    "        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # (batch_size, encoder_dim)\n",
    "\n",
    "        return attention_weighted_encoding, alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 192])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention=Attention(encoder_dim=192, decoder_dim=512, attention_dim=192)\n",
    "attention.encoder_att(enc_out).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderWithAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim=2048, dropout=0.5):\n",
    "        \"\"\"\n",
    "        :param attention_dim: size of attention network\n",
    "        :param embed_dim: embedding size\n",
    "        :param decoder_dim: size of decoder's RNN\n",
    "        :param vocab_size: size of vocabulary\n",
    "        :param encoder_dim: feature size of encoded images\n",
    "        :param dropout: dropout\n",
    "        \"\"\"\n",
    "        super(DecoderWithAttention, self).__init__()\n",
    "\n",
    "        self.encoder_dim = encoder_dim\n",
    "        self.attention_dim = attention_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.decoder_dim = decoder_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)  # attention network\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)  # embedding layer\n",
    "        self.dropout = nn.Dropout(p=self.dropout)\n",
    "        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)  # decoding LSTMCell\n",
    "        self.init_h = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial hidden state of LSTMCell\n",
    "        self.init_c = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial cell state of LSTMCell\n",
    "        self.f_beta = nn.Linear(decoder_dim, encoder_dim)  # linear layer to create a sigmoid-activated gate\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc = nn.Linear(decoder_dim, vocab_size)  # linear layer to find scores over vocabulary\n",
    "        self.init_weights()  # initialize some layers with the uniform distribution\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initializes some parameters with values from the uniform distribution, for easier convergence.\n",
    "        \"\"\"\n",
    "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "    def load_pretrained_embeddings(self, embeddings):\n",
    "        \"\"\"\n",
    "        Loads embedding layer with pre-trained embeddings.\n",
    "        :param embeddings: pre-trained embeddings\n",
    "        \"\"\"\n",
    "        self.embedding.weight = nn.Parameter(embeddings)\n",
    "\n",
    "    def fine_tune_embeddings(self, fine_tune=True):\n",
    "        \"\"\"\n",
    "        Allow fine-tuning of embedding layer? (Only makes sense to not-allow if using pre-trained embeddings).\n",
    "        :param fine_tune: Allow?\n",
    "        \"\"\"\n",
    "        for p in self.embedding.parameters():\n",
    "            p.requires_grad = fine_tune\n",
    "\n",
    "    def init_hidden_state(self, encoder_out):\n",
    "        \"\"\"\n",
    "        Creates the initial hidden and cell states for the decoder's LSTM based on the encoded images.\n",
    "        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n",
    "        :return: hidden state, cell state\n",
    "        \"\"\"\n",
    "#         mean_encoder_out = encoder_out.mean(dim=1)\n",
    "        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n",
    "        c = self.init_c(mean_encoder_out)\n",
    "        return h, c\n",
    "\n",
    "    def forward(self, encoder_out, encoded_captions, caption_lengths):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "        :param encoder_out: encoded images, a tensor of dimension (batch_size, enc_image_size, enc_image_size, encoder_dim)\n",
    "        :param encoded_captions: encoded captions, a tensor of dimension (batch_size, max_caption_length)\n",
    "        :param caption_lengths: caption lengths, a tensor of dimension (batch_size, 1)\n",
    "        :return: scores for vocabulary, sorted encoded captions, decode lengths, weights, sort indices\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = encoder_out.size(0)\n",
    "        encoder_dim = encoder_out.size(-1)\n",
    "        vocab_size = self.vocab_size\n",
    "\n",
    "        # Flatten image\n",
    "        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n",
    "        num_pixels = encoder_out.size(1)\n",
    "\n",
    "        # Sort input data by decreasing lengths; why? apparent below\n",
    "        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n",
    "        encoder_out = encoder_out[sort_ind]\n",
    "        encoded_captions = encoded_captions[sort_ind]\n",
    "\n",
    "        # Embedding\n",
    "        embeddings = self.embedding(encoded_captions)  # (batch_size, max_caption_length, embed_dim)\n",
    "\n",
    "        # Initialize LSTM state\n",
    "        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n",
    "\n",
    "        # We won't decode at the <end> position, since we've finished generating as soon as we generate <end>\n",
    "        # So, decoding lengths are actual lengths - 1\n",
    "        decode_lengths = (caption_lengths - 1).tolist()\n",
    "\n",
    "        # Create tensors to hold word predicion scores and alphas\n",
    "        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(device)\n",
    "        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(device)\n",
    "\n",
    "        # At each time-step, decode by\n",
    "        # attention-weighing the encoder's output based on the decoder's previous hidden state output\n",
    "        # then generate a new word in the decoder with the previous word and the attention weighted encoding\n",
    "        for t in range(max(decode_lengths)):\n",
    "            batch_size_t = sum([l > t for l in decode_lengths])\n",
    "            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],\n",
    "                                                                h[:batch_size_t])\n",
    "            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))  # gating scalar, (batch_size_t, encoder_dim)\n",
    "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
    "            h, c = self.decode_step(\n",
    "                torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),\n",
    "                (h[:batch_size_t], c[:batch_size_t]))  # (batch_size_t, decoder_dim)\n",
    "            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n",
    "            predictions[:batch_size_t, t, :] = preds\n",
    "            alphas[:batch_size_t, t, :] = alpha\n",
    "\n",
    "        return predictions, encoded_captions, decode_lengths, alphas, sort_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder=DecoderWithAttention(attention_dim=192, embed_dim=192, decoder_dim=512,\n",
    "                             vocab_size=len(word_map), encoder_dim=192)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, encoded_captions, decode_lengths, alphas, sort_ind=decoder(enc_out,\n",
    "                                                                        ds.__getitem__(0)[1].unsqueeze(0),ds.__getitem__(0)[2].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start>',\n",
       " '2',\n",
       " 'pieces',\n",
       " ',',\n",
       " 'reduced',\n",
       " 'spermatogenesis',\n",
       " 'present',\n",
       " ',',\n",
       " 'moderately',\n",
       " 'autolyzed',\n",
       " '<end>']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[idx2token[i] for i in ds.__getitem__(0)[1].numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.__getitem__(0)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 651])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.1258,  0.4658, -0.3618,  ..., -0.0680,  0.1996, -0.0493],\n",
       "          [-0.0965,  0.1615, -0.2198,  ..., -0.1077,  0.1472, -0.0969],\n",
       "          [-0.1618,  0.1001, -0.3327,  ..., -0.0561, -0.1123, -0.0299],\n",
       "          ...,\n",
       "          [-0.2500,  0.0999, -0.2300,  ..., -0.1294,  0.0654, -0.0958],\n",
       "          [-0.1324, -0.0689, -0.1759,  ..., -0.0176,  0.1441, -0.3371],\n",
       "          [-0.2659, -0.1541, -0.1540,  ...,  0.0032,  0.1916, -0.2877]]],\n",
       "        device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([[649,   1,   2,   3, 238,  62,  29,   3, 239, 234, 650]]),\n",
       " [10],\n",
       " tensor([[[1.],\n",
       "          [1.],\n",
       "          [1.],\n",
       "          [1.],\n",
       "          [1.],\n",
       "          [1.],\n",
       "          [1.],\n",
       "          [1.],\n",
       "          [1.],\n",
       "          [1.]]], device='cuda:0', grad_fn=<CopySlices>),\n",
       " tensor([0]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions, encoded_captions, decode_lengths, alphas, sort_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 1.10.0",
   "language": "python",
   "name": "pytorch-1.10.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
